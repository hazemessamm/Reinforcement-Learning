{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "import random\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "#tf.compat.v1.enable_control_flow_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experience replay is used because Neural Networks doesn't perform well on correlated data and reinforcement learning data\n",
    "#is very correlated, imagine that we have a car in x = 0.6 and y = 0.8 that's our current state\n",
    "#and the next state is x = 0.65 and y = 0.85 actually the two states are very correlated,\n",
    "#so we use experience replay to store the data of every state instead of throwing them away\n",
    "#then we take random sample everytime and that's will break the correlation because the states will be randomly sampled.\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, max_batch_size):\n",
    "        self.max_batch_size = max_batch_size #Specifying the maximum length of tuples that the queue can store\n",
    "        self.experience_buffer = deque(maxlen=self.max_batch_size) #creating double-ended queue\n",
    "    def sample(self, batch_size):\n",
    "        #sample randomly to avoid correlation\n",
    "        #note that random here is not related to numpy\n",
    "        return random.sample(self.experience_buffer, batch_size)\n",
    "    def store(self, sample):\n",
    "        #sample is stored as tuple\n",
    "        self.experience_buffer.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, epsilon=0.1, env_shape=None, action_shape=None, batch_size=32, gamma = 0.9, maxlen=10000):\n",
    "        self.epsilon = epsilon #exploration rate (the probability of choosing random action, this increases accuracy)\n",
    "        self.env_shape = env_shape #the shape of the information in each state\n",
    "        self.action_shape = action_shape #the number of actions available in the environment\n",
    "        self.model = self.create_model() #this function creates and compiles the model\n",
    "        self.experience = ExperienceReplay(maxlen) #creating an instance of ExperienceReplay with maximum length 10000\n",
    "        self.batch_size = batch_size #batch size for training\n",
    "        self.gamma = gamma #discount factor to avoid the agent from being short sighted or long sighted\n",
    "\n",
    "        \n",
    "    #this method uses self.epsilon to explore, in this example epsilon is 0.1 so the agent will choose random\n",
    "    #every 10 times\n",
    "    def e_greedy_step(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.action_shape-1)\n",
    "        else:\n",
    "            #choosing the maximum index which have the highest action value\n",
    "            return np.argmax(self.predict(state, is_batch=False))\n",
    "        \n",
    "    def create_model(self):\n",
    "        #the model is created using keras functional api\n",
    "        input_shape = tf.keras.layers.Input(self.env_shape) #Input layer which takes the shape\n",
    "        #Dense layer with relu activation function and 128 neuron so our matrix is [input_shape, 128] (hidden layer)\n",
    "        dense1 = tf.keras.layers.Dense(64, activation='relu')(input_shape)\n",
    "        #dense2 = tf.keras.layers.Dense(32, activation='relu')(dense1)\n",
    "        #output layer which output 2 probabilities which sum up to one and non-zero (1 probability for each action)\n",
    "        dense3 = tf.keras.layers.Dense(self.action_shape, activation='linear')(dense1) \n",
    "        #creating the model by Model method which maps from input_shape to dense2 which is the output function\n",
    "        model = tf.keras.models.Model(input_shape, dense3)\n",
    "        model.compile(loss=tf.keras.losses.MSE, optimizer=tf.keras.optimizers.Adam())\n",
    "        return model\n",
    "    \n",
    "    #this function is used for prediction, if we want to predict a batch of states or just one state\n",
    "    def predict(self, state, is_batch=False):\n",
    "        if is_batch:\n",
    "            return self.model.predict(state, self.batch_size)\n",
    "        else:\n",
    "            return self.model.predict(state.reshape(1, self.env_shape)).flatten()\n",
    "    \n",
    "    def optimise(self, states_x, states_y, epochs=1):\n",
    "        self.model.fit(states_x, states_y, epochs=epochs, verbose = 0)\n",
    "    def replay(self):\n",
    "        #if the buffer doesn't have enough data to train on then we don't need to continue the method\n",
    "        if len(self.experience.experience_buffer) < self.batch_size:\n",
    "            return\n",
    "        #getting the current batch which have constant size\n",
    "        current_batch = self.experience.sample(self.batch_size)\n",
    "        #if we are in terminal state it should be a vector of zeros with length of the state shape\n",
    "        terminal_state = [0 for i in range(self.env_shape)]\n",
    "        #stacking all the current states in current_states\n",
    "        current_states = np.array([s[0] for s in current_batch])\n",
    "        #stacking all the next states in next_states except if we are in terminal state we will replace None with the vector\n",
    "        #that we specified above\n",
    "        next_states = np.array([(terminal_state if s[3] is None else s[3]) for s in current_batch])\n",
    "        #predict batch of states to apply the q learning equation\n",
    "        q = self.predict(current_states, is_batch=True)\n",
    "        #predict batch of next states to apply the q learning equation\n",
    "        q_next = self.predict(next_states, is_batch=True)\n",
    "        \n",
    "        #np array it will be used for populating to train on it\n",
    "        x = np.zeros((self.batch_size, self.env_shape))\n",
    "        y = np.zeros((self.batch_size, self.action_shape))\n",
    "        for i in range(self.batch_size):\n",
    "            #getting every state\n",
    "            current_state = current_batch[i][0]\n",
    "            current_action = current_batch[i][1]\n",
    "            reward = current_batch[i][2]\n",
    "            next_state = current_batch[i][3]\n",
    "            \n",
    "            values = q[i]\n",
    "            if next_state is None:\n",
    "                values[current_action] = reward\n",
    "            else:\n",
    "                #the equation means that we are updating in the direction of the q target which is reward + self.gamma * max(q_next[i])\n",
    "                values[current_action] = reward + self.gamma * np.amax(q_next[i])\n",
    "            x[i] = current_state\n",
    "            y[i] = values\n",
    "        self.optimise(x, y, epochs=2) #after populating x and y we will modify our weights on them\n",
    "    def store_experience(self, sample):\n",
    "        self.experience.store(sample) #this function is used to store experiences in our Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, env_name=None, episods_num=100, render=False):\n",
    "        if env_name is not None:\n",
    "            self.env_name = env_name #getting the name of the environment\n",
    "            self.env = gym.make(self.env_name) #creating the environment\n",
    "            self.n_actions = self.env.action_space.n #getting the number of available action for that environment\n",
    "            self.env_shape = self.env.observation_space.shape[0] #getting how many information variables we will get from every state\n",
    "            self.render = render #boolean variable to check if we want to see the output or not\n",
    "            self.agent = DQNAgent(env_shape=self.env_shape, action_shape = self.n_actions, gamma=1) #creating an instance of the DQN agent\n",
    "            self.episods_num=episods_num\n",
    "        else:\n",
    "            raise Exception(\"env_name should not be None\")\n",
    "    def begin(self): #training function\n",
    "        for i in range(self.episods_num):\n",
    "            state = self.env.reset() #we should reset every time we start the environment\n",
    "            total_reward = 0 #variable to know the total reward after each episode\n",
    "            while True:\n",
    "                if self.render:\n",
    "                    self.env.render()\n",
    "                else:\n",
    "                    pass\n",
    "                action = self.agent.e_greedy_step(state) #getting the action from our policy\n",
    "                next_state, reward, done, _ = self.env.step(action) #applying the action in the environment\n",
    "                if done:\n",
    "                    next_state = None #check if done this will mean that we are in terminal state so next_state should be equal to None\n",
    "                self.agent.store_experience((state, action, reward, next_state)) #storing experience in our experience buffer\n",
    "                self.agent.replay() #learn the agent to perform better in the next episodes\n",
    "                state = next_state #updaing the state to next state if we are not in terminal state\n",
    "                total_reward += reward #increasing the total reward\n",
    "                if done:\n",
    "                    break\n",
    "            print(f'Total Reward {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\hazem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "environment = Environment('MountainCar-v0', render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -131.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -187.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -177.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -189.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n",
      "Total Reward -200.0\n"
     ]
    }
   ],
   "source": [
    "environment.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I got some help from this website: https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
